{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2b35dd",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afb1989",
   "metadata": {},
   "source": [
    "## Initialize library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afc03872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import pos_tag, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy\n",
    "from math import pow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef70676d",
   "metadata": {},
   "source": [
    "## Basic naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ffa8877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_to_list(dataframe):    # Convert dataframe into list\n",
    "    documents = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        tokens = nltk.word_tokenize(row['Comment']) # Tokenize each document\n",
    "        documents.append((tokens, row['Label']))    # Add the tokenized document into the list, paired with the label\n",
    "    return documents\n",
    "\n",
    "def feature_extraction(document, word_features):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))    # Create the list of stop words\n",
    "\n",
    "    filtered_sentence = [w for w in document if not w in stop_words] # Filter the sentence by removing stop words\n",
    "    document_words = set(filtered_sentence)     # Remove repeating word\n",
    "    features = {}\n",
    "    for word in word_features:  # Loop through all the word features\n",
    "        # Enter the feature (Extract the feature from each document) to the dictionary\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "def feature_selection(documents):\n",
    "    word_list = []\n",
    "    for docs, label in documents:   # Loop through all documents\n",
    "        word_list.extend(docs)  # Append all the word into the list\n",
    "    document_set = set(word_list)   # Remove repeating word\n",
    "    # Get the distribution of the word\n",
    "    all_words = nltk.FreqDist(word.lower()for word in document_set)\n",
    "    # Get the top 2000 most frequent words as feature\n",
    "    word_features = list(all_words)[:2000]\n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70ba4318",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('amazon_cells_labelled.csv')    # Read the file as dataframe\n",
    "docs = make_to_list(dataframe)  # Get the dataframe as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb1ec4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data points in training set = 800, Data points in test set = 200\n"
     ]
    }
   ],
   "source": [
    "feature = feature_selection(docs)   #Select the feature\n",
    "# Extract the selected features from documents\n",
    "featuresets = [(feature_extraction(d, feature), c) for (d, c) in docs]\n",
    "train_set, test_set = featuresets[200:], featuresets[:200]  # Split test and train data\n",
    "print(\"Data points in training set = {}, Data points in test set = {}\".format(len(train_set), len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36f6331e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is : 75.5%\n",
      "Most Informative Features\n",
      "         contains(works) = True                1 : 0      =     15.1 : 1.0\n",
      "         contains(great) = True                1 : 0      =      9.5 : 1.0\n",
      "         contains(money) = True                0 : 1      =      9.0 : 1.0\n",
      "         contains(price) = True                1 : 0      =      8.6 : 1.0\n",
      "         contains(happy) = True                1 : 0      =      7.7 : 1.0\n",
      "         contains(first) = True                0 : 1      =      7.6 : 1.0\n",
      "          contains(fine) = True                1 : 0      =      7.0 : 1.0\n",
      "          contains(best) = True                1 : 0      =      5.8 : 1.0\n",
      "         contains(calls) = True                0 : 1      =      5.6 : 1.0\n",
      "   contains(comfortable) = True                1 : 0      =      5.0 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "classifier1 = nltk.NaiveBayesClassifier.train(train_set) # Train the naive bayes classifier\n",
    "# Get the accuracy\n",
    "print(\"Accuracy is : {}%\".format(accuracy(classifier1, test_set)*100)) \n",
    "print(classifier1.show_most_informative_features(10))  # Get 10 of the most informative features in the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20af38d6",
   "metadata": {},
   "source": [
    "## Naive Bayes + Pos tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78c2d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for nltk pos tag groupings\n",
    "POS_TAG_GROUPINGS = {\n",
    "    \"JJ\": \"Adjective\", \"JJR\": \"Adjective\", \"JJS\": \"Adjective\",\n",
    "    \"NN\": \"Noun\", \"NNS\": \"Noun\", \"NNP\": \"Noun\", \"NNPS\": \"Noun\",\n",
    "    \"PRP\": \"Pronoun\", \"PRP$\": \"Pronoun\",\n",
    "    \"RB\": \"Adverb\", \"RBR\": \"Adverb\", \"RBS\": \"Adverb\",\n",
    "    \"VB\": \"Verb\", \"VBD\": \"Verb\", \"VBG\": \"Verb\", \"VBN\": \"Verb\", \"VBP\": \"Verb\", \"VBZ\": \"Verb\",\n",
    "    \"VH\": \"Verb\", \"VHD\": \"Verb\", \"VHG\": \"Verb\", \"VHN\": \"Verb\", \"VHP\": \"Verb\", \"VHZ\": \"Verb\",\n",
    "    \"VV\": \"Verb\", \"VVD\": \"Verb\", \"VVG\": \"Verb\", \"VVN\": \"Verb\", \"VVP\": \"Verb\", \"VVZ\": \"Verb\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0b31dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_to_list(dataframe): # Convert dataframe into list\n",
    "    documents = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        word_tokens = word_tokenize(row['Comment']) # Tokenize each document\n",
    "        tags = pos_tag(word_tokens) # Get the tag for each tokenized word\n",
    "        word_set = []\n",
    "        for word, tag in zip(word_tokens, tags):    # Loop through the tokens and tag pair\n",
    "            if tag[1] not in POS_TAG_GROUPINGS.keys():  # If the tag is not in the group we consider\n",
    "                # Append the word in a tuple together with the tag as 'Others'\n",
    "                word_set.append(tuple([word.lower(), 'Others']))\n",
    "            else:   # If the tag is in the group we consider\n",
    "                # Append the word in a tuple together with the tag group\n",
    "                word_set.append(tuple([word.lower(), POS_TAG_GROUPINGS[tag[1]]]))\n",
    "        # Append the whole set of row (document) into the main documents list\n",
    "        documents.append((word_set, row['Label']))\n",
    "    return documents\n",
    "\n",
    "def feature_extraction(document, word_features):    # Get the feature for the data row (document)\n",
    "    stop_words = set(stopwords.words('english'))    # Create the list of stop words\n",
    "    filtered_sentence = [(w,t) for w, t in document if not w in stop_words] # Filter the sentence by removing stop words\n",
    "    document_words = set(filtered_sentence)     # Remove repeating word set in the document\n",
    "    features = {}\n",
    "    for word_set in word_features:   # Loop through all the features selected\n",
    "        # Enter the feature (Extract the feature from each document) to the dictionary\n",
    "        features['{} as {}'.format(word_set[0], word_set[1])] = \\\n",
    "            (tuple([word_set[0], word_set[1]]) in document_words)\n",
    "    return features\n",
    "\n",
    "\n",
    "def feature_selection(documents):\n",
    "    word_list = []\n",
    "    for docs, label in documents:   # Loop through all document\n",
    "        word_list.extend(docs)  # Append all the word-tag pair\n",
    "    document_set = set(word_list)   # Remove repeating pair\n",
    "    all_words = FreqDist((word.lower(), tag) for word, tag in document_set) #Get the distribution\n",
    "    word_features = list(all_words)[:2000]  # Get the 2000 most common pair\n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12789686",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('amazon_cells_labelled.csv')    # Read the file as dataframe\n",
    "docs = make_to_list(dataframe)  # Get the dataframe as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f9e3e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data point in training set = 800, Data point in test set = 200\n"
     ]
    }
   ],
   "source": [
    "features = feature_selection(docs)      # Select the features to use\n",
    "# Extract the selected features from the documents\n",
    "featuresets = [(feature_extraction(d, features), c) for (d, c) in docs]\n",
    "train_set, test_set = featuresets[200:], featuresets[:200]  # Split the train and test data\n",
    "print(\"Data point in training set = {}, Data point in test set = {}\".format(len(train_set), len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "470e25e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is : 80.5%\n",
      "Most Informative Features\n",
      "           works as Verb = True                1 : 0      =     15.1 : 1.0\n",
      "  excellent as Adjective = True                1 : 0      =     14.4 : 1.0\n",
      "      great as Adjective = True                1 : 0      =     10.2 : 1.0\n",
      "           price as Noun = True                1 : 0      =      9.0 : 1.0\n",
      "           money as Noun = True                0 : 1      =      9.0 : 1.0\n",
      "      happy as Adjective = True                1 : 0      =      7.7 : 1.0\n",
      "           piece as Noun = True                0 : 1      =      7.6 : 1.0\n",
      "comfortable as Adjective = True                1 : 0      =      7.0 : 1.0\n",
      "       best as Adjective = True                1 : 0      =      6.6 : 1.0\n",
      "      first as Adjective = True                0 : 1      =      6.3 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "classifier2 = NaiveBayesClassifier.train(train_set)  # Train the naive bayes classifier\n",
    "print(\"Accuracy is : {}%\".format(accuracy(classifier2, test_set)*100))    # Get the accuracy\n",
    "print(classifier2.show_most_informative_features(10))    # Get 10 of the most informative features in the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b325e8b7",
   "metadata": {},
   "source": [
    "## Naive Bayes + Sentiment Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "738edc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score / weight for each group of tag\n",
    "POS_TAG_SCORING = {\n",
    "    \"Adjective\": 0.5,\n",
    "    \"Verb\": 0.333,\n",
    "    \"Noun\": 0.2,\n",
    "    \"Adverb\": 0,\n",
    "    \"Pronoun\": 0.2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64d0923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_to_list(dataframe): # Convert dataframe into list\n",
    "    documents = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        word_tokens = word_tokenize(row['Comment']) # Tokenize each document\n",
    "        tags = pos_tag(word_tokens) # Get the tag for each tokenized word\n",
    "        word_set = []\n",
    "        for word, tag in zip(word_tokens, tags):    # Loop through the tokens and tag pair\n",
    "            if tag[1] not in POS_TAG_GROUPINGS.keys():  # If the tag is not in the group we consider\n",
    "                # Append the word in a tuple together with the tag as 'Others'\n",
    "                word_set.append(tuple([word.lower(), 'Others']))\n",
    "            else:   # If the tag is in the group we consider\n",
    "                # Append the word in a tuple together with the tag group\n",
    "                word_set.append(tuple([word.lower(), POS_TAG_GROUPINGS[tag[1]]]))\n",
    "        # Append the whole set of row (document) into the main documents list\n",
    "        documents.append((word_set, row['Label']))\n",
    "    return documents\n",
    "\n",
    "\n",
    "def sentiment_polarity_dictionary_creation(documents):  # Create a sentiment polarity dictionary\n",
    "    list_of_all_words = []  # Prepare a list to store all the word set (Word with the group tag)\n",
    "    dictionary_count = {0: {}, 1: {}}   # Create a dictionary for their count based on classification label\n",
    "    for word_sets, label in documents:   # Loop through the whole data row (Documents)\n",
    "        non_repeat = set(word_sets)  # Remove repeating word set\n",
    "        for word_and_tag in non_repeat:    # Loop through the non repeating word set\n",
    "            list_of_all_words.append(word_and_tag)    # Append the word set into the main list\n",
    "            if word_and_tag not in dictionary_count[label]: # If the word set is not in the dictionary\n",
    "                # Make a dictionary for both classification with 0 as the initial count\n",
    "                dictionary_count[0][word_and_tag] = 0\n",
    "                dictionary_count[1][word_and_tag] = 0\n",
    "                dictionary_count[label][word_and_tag] = 1   # Set value as 1 for the label where that word is found\n",
    "            else:   # If it already existed\n",
    "                dictionary_count[label][word_and_tag] += 1    # Add 1 to the previous count value\n",
    "    non_repeating_list = set(list_of_all_words) # Remove repeating word set in the main list\n",
    "    # Create a polarity score dictionary\n",
    "    polarity_dictionary = polarity_calculation(dictionary_count, non_repeating_list)\n",
    "    return polarity_dictionary\n",
    "\n",
    "\n",
    "def polarity_calculation(dictionary_count, list_of_all_words):  # Create a polarity score dictionary\n",
    "    # Create the dictionary for all word set with 0 as the initial value\n",
    "    polarity_dictionary = {word_set: 0 for word_set in list_of_all_words}\n",
    "    for word, tag in list_of_all_words: # Loop through all the wordset in the documents\n",
    "        negative_count = dictionary_count[0][(word, tag)]   # Get the count of this word set appear in negative document\n",
    "        positive_count = dictionary_count[1][(word, tag)]   # Get the count of this word set appear in positive document\n",
    "        # Get the score for the group tag, 0 for those not in the POS_SCORING\n",
    "        tag_score = POS_TAG_SCORING[tag] if tag in POS_TAG_SCORING.keys() else 0\n",
    "        # Calculate the polarity score\n",
    "        polarity_score = (pow(tag_score, negative_count+1) - pow(tag_score, positive_count+1)) / (1-tag_score)\n",
    "        polarity_dictionary[(word, tag)] = polarity_score   # Insert the polarity score for the word set\n",
    "    return polarity_dictionary\n",
    "\n",
    "\n",
    "def feature_extraction(document, word_features):    # Get the feature for the data row (document)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))    # Create the list of stop words\n",
    "\n",
    "    filtered_sentence = [(w,t) for w, t in document if not w in stop_words] # Filter the sentence by removing stop words\n",
    "    document_words = set(filtered_sentence)     # Remove repeating word set in the document\n",
    "    features = {}\n",
    "    for word_set, value in word_features:   # Loop through all the features selected\n",
    "        # Enter the feature (Extract the feature from each document) to the dictionary\n",
    "        features['{} as {}'.format(word_set[0], word_set[1])] = \\\n",
    "            (tuple([word_set[0], word_set[1]]) in document_words)\n",
    "    return features\n",
    "\n",
    "\n",
    "def feature_selection(polarity_dictionary, feature_no = 1, positive_n = 750, negative_n = 850):\n",
    "    # Select features to use\n",
    "    # Sort the polarity scores in ascending order\n",
    "    sorted_polarity = sorted(polarity_dictionary.items(), key=lambda kv: (kv[1], kv[0]))\n",
    "    # Get the n_negative number of most negative polarity\n",
    "    most_negative_features = sorted_polarity[:negative_n]\n",
    "    print(\"Highest in most negative features selected \", most_negative_features[-1])\n",
    "    # Get the n_positive number of most positive polarity\n",
    "    most_positive_features = sorted_polarity[-positive_n:]\n",
    "    print(\"Lowest in most positive features selected \", most_positive_features[0])\n",
    "\n",
    "    features1 = []\n",
    "    features1.extend(most_negative_features)    # Add the negative features into the main list of features\n",
    "    features1.extend(most_positive_features)    # Add the positive features into the main list of features\n",
    "    print(\"Features 1 ({} most positive + {} most negative: {}\"\n",
    "          .format(len(most_positive_features), len(most_negative_features), len(features1)))\n",
    "    # Feature 2, get all polarity that is not 0 as features\n",
    "    features2 = [(word_set, score) for word_set, score in sent_dict.items() if score != 0]\n",
    "    print(\"Features 2 (Non zero polarity):\", len(features2))\n",
    "    features_selected = []\n",
    "    if feature_no == 1:  # Select features 1 as the features\n",
    "        features_selected = features1\n",
    "        print(\"Feature 1 is selected\")\n",
    "    elif feature_no == 2:   # Select features 2 as the features\n",
    "        features_selected = features2\n",
    "        print(\"Feature 2 is selected\")\n",
    "    return features_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33dec9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('amazon_cells_labelled.csv')    # Read the file as dataframe\n",
    "docs = make_to_list(dataframe)  # Get the dataframe as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "610a02f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest in most negative features selected  (('kind', 'Noun'), -0.049600000000000005)\n",
      "Lowest in most positive features selected  (('car', 'Noun'), 1.5994880000000004e-05)\n",
      "Features 1 (750 most positive + 850 most negative: 1600\n",
      "Features 2 (Non zero polarity): 1702\n",
      "Feature 1 is selected\n",
      "Data point in training set = 800, Data point in test set = 200\n"
     ]
    }
   ],
   "source": [
    "sent_dict = sentiment_polarity_dictionary_creation(docs)    # Get the sentiment polarity score\n",
    "features = feature_selection(sent_dict, 1)      # Select the features to use\n",
    "# Extract the selected features from the documents\n",
    "featuresets = [(feature_extraction(set, features), c) for (set, c) in docs]\n",
    "train_set, test_set = featuresets[200:], featuresets[:200]  # Split the train and test data\n",
    "print(\"Data point in training set = {}, Data point in test set = {}\".format(len(train_set), len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e60e2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is : 85.5%\n",
      "Most Informative Features\n",
      "           works as Verb = True                1 : 0      =     15.1 : 1.0\n",
      "  excellent as Adjective = True                1 : 0      =     14.4 : 1.0\n",
      "      great as Adjective = True                1 : 0      =     10.2 : 1.0\n",
      "           price as Noun = True                1 : 0      =      9.0 : 1.0\n",
      "           money as Noun = True                0 : 1      =      9.0 : 1.0\n",
      "      happy as Adjective = True                1 : 0      =      7.7 : 1.0\n",
      "comfortable as Adjective = True                1 : 0      =      7.0 : 1.0\n",
      "       best as Adjective = True                1 : 0      =      6.6 : 1.0\n",
      "      first as Adjective = True                0 : 1      =      6.3 : 1.0\n",
      "             buy as Verb = True                0 : 1      =      5.6 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "classifier3 = NaiveBayesClassifier.train(train_set)  # Train the naive bayes classifier\n",
    "print(\"Accuracy is : {}%\".format(accuracy(classifier3, test_set)*100))    # Get the accuracy\n",
    "print(classifier3.show_most_informative_features(10))    # Get 10 of the most informative features in the classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
